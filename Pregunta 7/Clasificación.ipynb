{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "boston.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.data\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "algoritmo = DecisionTreeClassifier(criterion = 'entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[144  93  19 105  72  56  85  18  87  73  91 124  36 158  57 115  78 162\n",
      "  10  20 175 108  42  14  62 169  79 170  92 117  30  38 171 172  81  88\n",
      " 148 177  40  98  99 135 100  25  41  79 140 103 197 106 149  88  49 141\n",
      "  46   1  28 142 152  66  66  49 137  84  49  28 123 116  93   2 135 194\n",
      " 195  80  77 122 122  34   6 163  19  27  66 146  51 149  80 123  31 125\n",
      " 192 129  11 141  86  77 104 118 179 103 125  41 113 165 138 120  67 136\n",
      "  31 117 106 178  85 106  58 196  95 166 108 124 111  99  93 153 156 150\n",
      " 171  81  99  90 105 100 197  62 127  51  23  50 105 128 129  70  61  91\n",
      " 133 131 124  59 130  86 116 160 139   6  93 160  71 142 185  84  15  39\n",
      " 155 187 182 163 106  92  88  86  74 110 113  91 141 183  46 106   8 154\n",
      "  28 112 100  82 133  94  25 116 102  40 186  85 147 184  97 117  60  37\n",
      " 153  75 133 197 116  64  89 124   0 101  82 107 109  72  77 132   4  75\n",
      " 133  37 131 115  28 119  39  33  63  25 133 159  73  79 131  17 118  99\n",
      " 111  43 130 132  94 137 108  29  68 175 177 104 103  47 197 114  59  97\n",
      "  79 197  81  13 161  35 114 190 100  38  21  43  82 188 114   9 122  18\n",
      "  66  73 127  15  70 111  34  57  93 151  16 123  51 154  84 118  55 164\n",
      " 157  54 197 193 103 116  90 140 122  34 116  35  63  84  89 110 150  91\n",
      "  75  87   1  83  74 126 173 176 121  53  54 181  43  65 145   2 103   3\n",
      "  44  26 197  88 197 189  31 161 197 133 197 106  72 141 113 115  32 101\n",
      " 132  77  32  22 108  87  49 197 180 169  97  52  87 111  62  44 100 116\n",
      "  82 137 114  69  20  45 168 125   7 133 197   5 197 124 152  12   2 197\n",
      " 103  90  95   8 167  95 108 191 197  96  81  37  48 128 110  40 174 134\n",
      "  60  76  88  99  24 168 143  55]\n",
      "continuous\n",
      "multiclass\n",
      "continuous\n",
      "multiclass\n",
      "multiclass\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "boston_encoded = lab_enc.fit_transform(y_train)\n",
    "boston_encoded = lab_enc.fit_transform(y_train)\n",
    "print(boston_encoded)\n",
    "print(utils.multiclass.type_of_target(y_train))\n",
    "print(utils.multiclass.type_of_target(y_train.astype('int')))\n",
    "print(utils.multiclass.type_of_target(y_test))\n",
    "print(utils.multiclass.type_of_target(y_test.astype('int')))\n",
    "print(utils.multiclass.type_of_target(boston_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier(criterion = 'entropy')\n",
    "dtc.fit(X_train, boston_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dtc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[24.5 11.5 15.4  8.4 18.5 32.5 10.2 31.1 13.4 24.8 24.4 19.7 23.  26.2\n 28.1 20.6 30.1 19.9  8.8 42.8 22.3 36.4  5.  21.6 17.5 36.2 18.8 22.\n 21.7 22.7 22.8 24.  10.4 36.1 23.4 22.4 19.6 19.7  8.7 17.8 34.9 19.4\n 36.5 32.4 19.5 25.2 29.9 17.1 22.6 13.3 25.  28.7 10.8 20.9 16.7  8.8\n 19.  19.5 16.2  6.3 18.2 48.3 20.3 13.8 14.2 21.1 24.3 10.2 23.8 23.6\n 28.4 19.3 13.6 41.3  5.6 30.7 24.  15.  10.5 28.7 18.3 22.4 12.6 16.2\n 15.6 24.4 20.5 17.2 19.4 19.5 16.  37.  23.3 35.2 20.  15.2 13.8 50.\n 23.2 25.3 36.2 13.3].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-cff09af42295>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \"\"\"\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mdtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    355\u001b[0m         \"\"\"\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    428\u001b[0m         \"\"\"\n\u001b[0;32m    429\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tree_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;34m\"\"\"Validate X whenever one tries to predict, apply, predict_proba\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m             if issparse(X) and (X.indices.dtype != np.intc or\n\u001b[0;32m    393\u001b[0m                                 X.indptr.dtype != np.intc):\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    519\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[24.5 11.5 15.4  8.4 18.5 32.5 10.2 31.1 13.4 24.8 24.4 19.7 23.  26.2\n 28.1 20.6 30.1 19.9  8.8 42.8 22.3 36.4  5.  21.6 17.5 36.2 18.8 22.\n 21.7 22.7 22.8 24.  10.4 36.1 23.4 22.4 19.6 19.7  8.7 17.8 34.9 19.4\n 36.5 32.4 19.5 25.2 29.9 17.1 22.6 13.3 25.  28.7 10.8 20.9 16.7  8.8\n 19.  19.5 16.2  6.3 18.2 48.3 20.3 13.8 14.2 21.1 24.3 10.2 23.8 23.6\n 28.4 19.3 13.6 41.3  5.6 30.7 24.  15.  10.5 28.7 18.3 22.4 12.6 16.2\n 15.6 24.4 20.5 17.2 19.4 19.5 16.  37.  23.3 35.2 20.  15.2 13.8 50.\n 23.2 25.3 36.2 13.3].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "precision = precision_score(y_test, y_pred)\n",
    "precision\n",
    "\n",
    "print(utils.multiclass.type_of_target(y_test))\n",
    "print(utils.multiclass.type_of_target(y_pred))\n",
    "print(utils.multiclass.type_of_target(y_test.astype('int')))\n",
    "print(utils.multiclass.type_of_target(y_pred.astype('int')))\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.model_selection import KFold\n",
    "dtc.score(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "la función de KFold que nos ayudará a crear varios subgrupos con nuestros datos de entrada para validar \n",
    "y valorar los árboles con diversos niveles de profundidad.\n",
    "\"\"\"\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "cv = KFold(n_splits=5) # Numero deseado de \"folds\" que haremos\n",
    "accuracies = list()\n",
    "max_attributes = len(list(boston))\n",
    "depth_range = range(1, max_attributes + 1)\n",
    "\n",
    "# Testearemos la profundidad de 1 a cantidad de atributos +1\n",
    "for depth in depth_range:\n",
    "    fold_accuracy = []\n",
    "    tree_model = DecisionTreeClassifier(criterion='entropy',\n",
    "                                             min_samples_split=20,\n",
    "                                             min_samples_leaf=5,\n",
    "                                             max_depth = depth,\n",
    "                                             class_weight={1:3.5})\n",
    "    for train_fold, valid_fold in cv.split(df):\n",
    "        f_train = boston.loc[train_fold] \n",
    "        f_valid = boston.loc[valid_fold] \n",
    " \n",
    "        model = tree_model.fit(X = f_train.drop(['compra_videojuego_si'], axis=1), \n",
    "                               y = f_train[\"compra_videojuego_si\"]) \n",
    "        valid_acc = model.score(X = f_valid.drop(['compra_videojuego_si'], axis=1), \n",
    "                                y = f_valid[\"compra_videojuego_si\"]) # calculamos la precision con el segmento de validacion\n",
    "        fold_accuracy.append(valid_acc)\n",
    " \n",
    "    avg = sum(fold_accuracy)/len(fold_accuracy)\n",
    "    accuracies.append(avg)\n",
    "    \n",
    "# Mostramos los resultados obtenidos\n",
    "df = pd.DataFrame({\"Max Depth\": depth_range, \"Average Accuracy\": accuracies})\n",
    "df = df[[\"Max Depth\", \"Average Accuracy\"]]\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.5, 11.5, 15.4,  8.4, 18.5, 32.5, 10.2, 31.1, 13.4, 24.8, 24.4,\n",
       "       19.7, 23. , 26.2, 28.1, 20.6, 30.1, 19.9,  8.8, 42.8, 22.3, 36.4,\n",
       "        5. , 21.6, 17.5, 36.2, 18.8, 22. , 21.7, 22.7, 22.8, 24. , 10.4,\n",
       "       36.1, 23.4, 22.4, 19.6, 19.7,  8.7, 17.8, 34.9, 19.4, 36.5, 32.4,\n",
       "       19.5, 25.2, 29.9, 17.1, 22.6, 13.3, 25. , 28.7, 10.8, 20.9, 16.7,\n",
       "        8.8, 19. , 19.5, 16.2,  6.3, 18.2, 48.3, 20.3, 13.8, 14.2, 21.1,\n",
       "       24.3, 10.2, 23.8, 23.6, 28.4, 19.3, 13.6, 41.3,  5.6, 30.7, 24. ,\n",
       "       15. , 10.5, 28.7, 18.3, 22.4, 12.6, 16.2, 15.6, 24.4, 20.5, 17.2,\n",
       "       19.4, 19.5, 16. , 37. , 23.3, 35.2, 20. , 15.2, 13.8, 50. , 23.2,\n",
       "       25.3, 36.2, 13.3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
